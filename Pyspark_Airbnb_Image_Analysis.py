# -*- coding: utf-8 -*-
"""PySpark_Airbnb_Image_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kfUtlZf53brPOm2Dbyf8VsPb1ZGH35VG
"""

!pip install tensorflow
!pip install PySpark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

import pandas as pd
from PIL import Image
from io import BytesIO
import requests
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

"""# Preparing The Data"""

file_paths = [
    '/content/Image Analysis Dataset.csv'
]
spark = SparkSession.builder.appName("Image Analysis").getOrCreate()

# Define the schema for the DataFrame
listings_schema = StructType([
    StructField("id", StringType(), True),
    StructField("picture_url", StringType(), True),
    StructField("property_type", StringType(), True)
])

# List to store PySpark DataFrames
spark_dfs = []

# Loop through each file, read the data, and create a PySpark DataFrame
for file_path in file_paths:
    current_spark_df = spark.read.csv(file_path, schema=listings_schema, header=True)
    spark_dfs.append(current_spark_df)

# Concatenate PySpark DataFrames
spark_data = spark_dfs[0] if len(spark_dfs) == 1 else spark_dfs[0].union(spark_dfs[1])

# Drop rows with all null values
spark_data = spark_data.dropna(how='all')

# Show the first 5 rows of the DataFrame
spark_data.show(5)

data = spark_data.toPandas()

print(data.head(5))

"""## Creating training and testing folder from the property_url


"""

import os
import requests
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split

data = data.dropna()

# List to store failed IDs
failed_ids = []

# Function to download and process images
def download_and_process_images(url, property_type, index, output_folder):
    property_type_dir = os.path.join(output_folder, property_type)
    os.makedirs(property_type_dir, exist_ok=True)

    response = requests.get(url)

    if response.status_code == 200:
        try:
            # Attempt to open the content as an image
            image = Image.open(BytesIO(response.content))
            # Resize if needed
            # image = image.resize((50, 50))
            if image.mode == 'RGBA':
                image = image.convert('RGB')
            image.save(os.path.join(property_type_dir, f"image_{index}.jpg"))
        except Exception as e:
            print(f"Error processing image from {url}: {str(e)}")
            failed_ids.append(index)
    else:
        print(f"Failed to download image from {url}")
        failed_ids.append(index)

# Split the filtered data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Create folders for training and testing
train_folder = "training_images"
test_folder = "testing_images"

os.makedirs(train_folder, exist_ok=True)
os.makedirs(test_folder, exist_ok=True)

# Download and process images for training set
for train_index, train_row in train_data.iterrows():
    download_and_process_images(train_row["picture_url"], train_row["property_type"], train_index, train_folder)

# Download and process images for testing set
for test_index, test_row in test_data.iterrows():
    download_and_process_images(test_row["picture_url"], test_row["property_type"], test_index, test_folder)

# Create a DataFrame with failed IDs
failed_ids_df = pd.DataFrame(failed_ids, columns=["Failed IDs"])
print("The following Airbnb Listings have denied Access:")
print(failed_ids_df)
print("Labeled image dataset created, organized, and split into training and testing sets.")

import tensorflow as tf

img_height,img_width=180,180
batch_size=35
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  '/content/training_images', labels= 'inferred',
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  '/content/training_images', labels= 'inferred',
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(6):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

"""# Training The Model

"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.layers import Dropout

resnet_model = Sequential()

pretrained_model= tf.keras.applications.ResNet50(include_top=False,
                   input_shape=(180,180,3),
                   pooling='avg',classes=3,
                   weights='imagenet')
for layer in pretrained_model.layers:
        layer.trainable=False

resnet_model.add(pretrained_model)
resnet_model.add(Flatten())
resnet_model.add(Dense(512, activation='relu'))
resnet_model.add(Dense(256, activation='relu'))  # Added additional dense layer
resnet_model.add(Dense(5, activation='softmax'))

resnet_model.summary()

resnet_model.compile(optimizer=Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

for data, labels in train_ds.take(1):  # Taking the first batch as an example here
  print("Batch of images shape:", data.shape) # 'data' is the batch of images and 'labels' is the batch of labels
  print("Batch of labels shape:", labels.shape)

epochs=8
history = resnet_model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

"""# Evaluating The Model"""

import cv2
import numpy as np
from tensorflow.keras.models import load_model
import os
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming 'Entire Condo' and 'Houseboat' are the only relevant classes
relevant_classes = ['Entire condo', 'Houseboat'] #For the simplicity of our model we have considered 2 classes

# Define the path to the folder containing test images
test_images_dir = '/content/testing_images'
class_labels = sorted(os.listdir(test_images_dir))

# Load and preprocess each image, make predictions, and calculate accuracy
results = []
y_true = []
y_pred = []

for relevant_class in class_labels:
    class_dir = os.path.join(test_images_dir, relevant_class)

    for image_filename in os.listdir(class_dir):
        image_path = os.path.join(class_dir, image_filename)

        # Read the image from the file path
        image = cv2.imread(image_path)

        # Check if the image was successfully loaded
        if image is not None:
            # Define the target dimensions
            img_height = 180
            img_width = 180

            # Resize the image
            image_resized = cv2.resize(image, (img_width, img_height))

            # Expand the dimensions to create a batch with a single image
            image = image_resized.reshape(1, img_height, img_width, 3)

            # Make predictions for the image
            predictions = resnet_model.predict(image)

            # Map numerical predictions to class labels
            predicted_class = class_labels[np.argmax(predictions)]

            # Map numerical predictions to class labels

            # Use the 'relevant_class' as the true label
            true_label = relevant_class  # Using the folder name as the true label

            # Calculate accuracy for the image (1 for correct, 0 for incorrect)
            accuracy = 1 if predicted_class == true_label else 0
            y_true.append(true_label)
            y_pred.append(predicted_class)
            results.append((image_filename, predicted_class, true_label, accuracy))
        else:
            print(f"Failed to load the image from {image_path}")

# Print the results
for image_filename, predicted_class, true_label, accuracy in results:
    print(f"Image: {image_filename}, Predicted Class: {predicted_class}, True Class: {true_label}, Accuracy: {accuracy}")

# Calculate the overall accuracy for all test images
overall_accuracy = sum(result[3] for result in results) / len(results)
print(f"Overall Accuracy: {overall_accuracy * 100:.2f}%")

"""##Confusion Matrix

"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_true, y_pred, labels=relevant_classes)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=relevant_classes)
disp.plot(cmap=plt.cm.Blues)
plt.show()